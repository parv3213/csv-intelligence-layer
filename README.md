# CSV Intelligence Layer

> **âš ï¸ Beta Status**: This project is currently in beta. Features and APIs may change.

A production-grade data ingestion and normalization service that treats CSVs as hostile input and processes them through a deterministic, multi-stage pipeline.

## ğŸš€ Live Demo

[![Try Live Demo](https://img.shields.io/badge/Try%20Live%20Demo-4285F4?style=for-the-badge&logo=vercel&logoColor=white)](https://csv-intelligence.vercel.app/)
[![Watch Demo Video](https://img.shields.io/badge/Watch%20Demo%20Video-FF0000?style=for-the-badge&logo=youtube&logoColor=white)](https://youtu.be/YmJzh_EW9fU)

- **Frontend**: [https://csv-intelligence.vercel.app/](https://csv-intelligence.vercel.app/)
- **Backend API**: [https://csv-intelligence-layer-production.up.railway.app](https://csv-intelligence-layer-production.up.railway.app)

## Overview

This service functions like a **compiler for data** â€” inferring schema, reconciling with user-defined expectations, normalizing types, validating constraints, and producing explainable outputs.

### Key Features

- **Safe Streaming Parser**: Handles malformed CSVs, detects encoding/delimiters
- **Type Inference**: Automatically detects column types with confidence scores
- **Schema Reconciliation**: Maps source columns to canonical schema using heuristics
- **Human-in-the-Loop**: Pauses pipeline on ambiguity, exposes decisions via API
- **Decision Persistence**: Reuses mapping decisions for future ingestions
- **Explainability**: Full audit trail of every decision made

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    PARSE    â”‚ â”€â”€â–¶ â”‚    INFER    â”‚ â”€â”€â–¶ â”‚     MAP     â”‚ â”€â”€â–¶ â”‚  VALIDATE   â”‚ â”€â”€â–¶ â”‚   OUTPUT    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                                              â–¼
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚  HUMAN REVIEW   â”‚
                                    â”‚   (if needed)   â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Tech Stack

### Backend
- **Runtime**: Node.js 20+, TypeScript
- **API**: Fastify with Zod validation & Swagger/OpenAPI docs
- **Queue**: BullMQ + Redis
- **Database**: PostgreSQL + Drizzle ORM
- **Storage**: Local filesystem (dev) / S3-ready (production)
- **AI**: OpenAI integration for ambiguous column mapping

### Frontend
- **Framework**: React 18 + TypeScript + Vite
- **UI**: TailwindCSS + shadcn/ui components
- **State**: Zustand for history management
- **API Client**: Native fetch with type-safe interfaces
- **Deployment**: Vercel with Analytics

## Getting Started

### Prerequisites

- Node.js 20+
- Docker & Docker Compose
- pnpm (recommended) or npm

### Setup

```bash
# Clone and install
git clone <repo-url>
cd csv-intelligence-layer
pnpm install

# Copy environment file
cp .env.example .env

# Start infrastructure (Postgres, Redis)
pnpm docker:up

# Generate database migrations
pnpm db:generate

# Run migrations
pnpm db:migrate

# Start development server (in separate terminals)
pnpm dev          # API server
pnpm worker:dev   # Background workers
```

### Frontend Development

```bash
# Navigate to frontend directory
cd frontend

# Install dependencies
pnpm install

# Copy environment file and configure API URL
cp .env.example .env
# Edit .env and set VITE_API_URL to your backend URL

# Start development server
pnpm dev

# Build for production
pnpm build

# Preview production build
pnpm preview
```

The frontend will be available at `http://localhost:5173` and includes:
- Interactive CSV upload playground
- Schema management interface
- Real-time pipeline status tracking
- Decision review panel for ambiguous mappings
- Ingestion history viewer

### API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/schemas` | Create a canonical schema |
| `GET` | `/schemas` | List all schemas |
| `GET` | `/schemas/:id` | Get schema by ID |
| `POST` | `/ingestions?schemaId={uuid}` | Upload CSV and start pipeline |
| `GET` | `/ingestions/:id` | Get ingestion status and results |
| `GET` | `/ingestions/:id/review` | Get pending decisions (awaiting_review status) |
| `POST` | `/ingestions/:id/resolve` | Submit human decisions to resume pipeline |
| `GET` | `/ingestions/:id/output?format=csv\|json` | Download cleaned data (CSV or JSON) |
| `GET` | `/ingestions/:id/decisions` | Get decision audit log |
| `GET` | `/health` | Health check |
| `GET` | `/docs` | Interactive Swagger API documentation |

### Example Usage

```bash
# 1. Create a canonical schema
curl -X POST http://localhost:3000/schemas \
  -H "Content-Type: application/json" \
  -d '{
    "name": "customers",
    "version": "1.0.0",
    "columns": [
      {"name": "email", "type": "email", "required": true},
      {"name": "name", "type": "string", "required": true},
      {"name": "signup_date", "type": "date"}
    ]
  }'

# 2. Upload a CSV for ingestion
curl -X POST "http://localhost:3000/ingestions?schemaId=<schema-id>" \
  -F "file=@customers.csv"

# 3. Check status
curl http://localhost:3000/ingestions/<ingestion-id>

# 4. If awaiting_review, resolve ambiguities
curl -X POST http://localhost:3000/ingestions/<id>/resolve \
  -H "Content-Type: application/json" \
  -d '{
    "decisions": [
      {"sourceColumn": "user_email", "targetColumn": "email"}
    ]
  }'

# 5. Download cleaned output
curl -O http://localhost:3000/ingestions/<id>/output
```

## Development

### Project Structure

```
.
â”œâ”€â”€ frontend/              # React frontend application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/   # UI components (schema, pipeline, upload)
â”‚   â”‚   â”œâ”€â”€ pages/        # Route pages (Home, Playground, Docs, About)
â”‚   â”‚   â”œâ”€â”€ lib/          # API client, config, utilities
â”‚   â”‚   â”œâ”€â”€ hooks/        # React hooks for data fetching
â”‚   â”‚   â”œâ”€â”€ stores/       # Zustand state management
â”‚   â”‚   â””â”€â”€ types/        # TypeScript type definitions
â”‚   â””â”€â”€ vercel.json       # Vercel deployment config
â”‚
â”œâ”€â”€ src/                   # Backend application
â”‚   â”œâ”€â”€ api/              # Fastify route handlers
â”‚   â”‚   â”œâ”€â”€ health.ts
â”‚   â”‚   â”œâ”€â”€ ingestions.ts
â”‚   â”‚   â””â”€â”€ schemas.ts
â”‚   â”œâ”€â”€ db/               # Database schema and connection
â”‚   â”‚   â”œâ”€â”€ index.ts
â”‚   â”‚   â””â”€â”€ schema.ts
â”‚   â”œâ”€â”€ services/         # Business logic
â”‚   â”‚   â”œâ”€â”€ column-mapping.ts    # AI-powered column mapping
â”‚   â”‚   â”œâ”€â”€ csv-parser.ts        # Streaming CSV parser
â”‚   â”‚   â”œâ”€â”€ storage.ts           # Storage abstraction layer
â”‚   â”‚   â””â”€â”€ type-inference.ts    # Column type detection
â”‚   â”œâ”€â”€ workers/          # BullMQ job processors
â”‚   â”‚   â”œâ”€â”€ index.ts             # Worker orchestration
â”‚   â”‚   â”œâ”€â”€ parse.worker.ts      # CSV parsing stage
â”‚   â”‚   â”œâ”€â”€ infer.worker.ts      # Type inference stage
â”‚   â”‚   â”œâ”€â”€ map.worker.ts        # Column mapping stage
â”‚   â”‚   â”œâ”€â”€ validate.worker.ts   # Data validation stage
â”‚   â”‚   â”œâ”€â”€ output.worker.ts     # Output generation stage
â”‚   â”‚   â””â”€â”€ queues.ts            # Queue definitions
â”‚   â”œâ”€â”€ types/            # TypeScript types and Zod schemas
â”‚   â”œâ”€â”€ utils/            # Utilities (logger, waitForDeps)
â”‚   â”œâ”€â”€ config.ts         # Environment configuration
â”‚   â”œâ”€â”€ index.ts          # API server entry point
â”‚   â”œâ”€â”€ prod-entry.ts     # Production entry point
â”‚   â””â”€â”€ server.ts         # Fastify server setup
â”‚
â””â”€â”€ docker-compose.yml    # Local dev infrastructure
```

### Running Tests

```bash
pnpm test           # Watch mode
pnpm test:run       # Single run
pnpm test:coverage  # With coverage
```

### Database

```bash
pnpm db:generate    # Generate migrations from schema
pnpm db:migrate     # Apply migrations
pnpm db:studio      # Open Drizzle Studio (visual editor)
```

## Roadmap

### âœ… Completed
- [x] Full 5-stage pipeline (parse â†’ infer â†’ map â†’ validate â†’ output)
- [x] All worker implementations (BullMQ-based)
- [x] OpenAI integration for intelligent column mapping
- [x] Human-in-the-loop review system
- [x] Decision audit logging
- [x] Interactive web UI (React + TailwindCSS)
- [x] Swagger/OpenAPI documentation
- [x] Multi-format output (CSV, JSON)
- [x] Production deployment (Railway + Vercel)
- [X] Complete S3 storage implementation for production scale

### ğŸš§ In Progress / Planned
- [ ] Webhook notifications for pipeline completion
- [ ] Comprehensive test suite (unit + integration)
- [ ] Advanced validation rules engine
- [ ] Batch ingestion support
- [ ] Rate limiting and API key authentication
- [ ] Data quality scoring and reports

## License

MIT
